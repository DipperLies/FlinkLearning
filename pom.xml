<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.apache.flink</groupId>
    <artifactId>flink-quickstart-java</artifactId>
    <version>1.0-SNAPSHOT</version>

    <repositories>
        <repository>
            <id>Ali</id>
            <name>ali Releases</name>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        </repository>
        <repository>
            <id>maven-hadoop</id>
            <name>Hadoop Releases</name>
            <url>http://repo1.maven.org/maven2/</url>
        </repository>

        <repository>
            <id>csot-center-repo</id>
            <name>csot-center-repo</name>
            <url>http://10.108.240.122:8077/nexus/content/repositories/BigData</url>
        </repository>

        <repository>
            <id>cloudera-repos</id>
            <name>Cloudera Repos</name>
            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
        </repository>

        <repository>
            <id>SparkPackagesRepo</id>
            <name>spark-redis</name>
            <url>http://dl.bintray.com/spark-packages/maven</url>
        </repository>


    </repositories>

    <properties>
        <flink.version>1.9.0</flink.version>
        <hadoop.version>2.6.0-cdh5.16.1</hadoop.version>
        <slf4j.version>1.7.7</slf4j.version>
        <log4j.version>1.2.17</log4j.version>
        <flink.format.parquet.version>1.10.0</flink.format.parquet.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
        <log4j.configuration>log4j-test.properties</log4j.configuration>
        <flink.shaded.version>6.0</flink.shaded.version>
        <java.version>1.8</java.version>
        <slf4j.version>1.7.15</slf4j.version>
        <log4j.version>1.2.17</log4j.version>
        <!-- Overwrite default values from parent pom.
             Intellij is (sometimes?) using those values to choose target language level
             and thus is changing back to java 1.6 on each maven re-import -->
        <maven.compiler.source>${java.version}</maven.compiler.source>
        <maven.compiler.target>${java.version}</maven.compiler.target>
        <scala.macros.version>2.1.0</scala.macros.version>
        <!-- Default scala versions, must be overwritten by build profiles, so we set something
        invalid here -->
        <scala.version>2.11.12</scala.version>
        <scala.binary.version>2.11</scala.binary.version>
        <chill.version>0.7.6</chill.version>
        <zookeeper.version>3.4.10</zookeeper.version>
        <curator.version>2.12.0</curator.version>
        <jackson.version>2.7.9</jackson.version>
        <metrics.version>3.1.5</metrics.version>
        <prometheus.version>0.3.0</prometheus.version>
        <avro.version>1.8.2</avro.version>
        <junit.version>4.12</junit.version>
        <mockito.version>2.21.0</mockito.version>
        <powermock.version>2.0.0-RC.4</powermock.version>
        <hamcrest.version>1.3</hamcrest.version>
        <japicmp.skip>false</japicmp.skip>
        <project.version>1.8.0</project.version>
        <flink.convergence.phase>validate</flink.convergence.phase>
        <!--
            Keeping the MiniKDC version fixed instead of taking hadoop version dependency
            to support testing Kafka, ZK etc., modules that does not have Hadoop dependency
            Starting Hadoop 3, org.apache.kerby will be used instead of MiniKDC. We may have
            to revisit the impact at that time.
        -->
        <minikdc.version>2.7.2</minikdc.version>
        <generated.docs.dir>./docs/_includes/generated</generated.docs.dir>
    </properties>


    <dependencies>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>${slf4j.version}</version>
<!--            <scope>provided</scope>-->
        </dependency>

        <!-- Flink dependencies -->

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-core</artifactId>
            <version>${flink.version}</version>
<!--            <scope>provided</scope>-->
        </dependency>

        <!-- Parquet Dependencies -->

        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-hadoop</artifactId>
            <version>${flink.format.parquet.version}</version>
        </dependency>

        <!-- Hadoop is needed by Parquet -->
<!--        <dependency>-->
<!--            <groupId>org.apache.flink</groupId>-->
<!--            <artifactId>flink-shaded-hadoop2</artifactId>-->
<!--            <version>${hadoop.version}-${flink.version}</version>-->
<!--            <scope>provided</scope>-->
<!--        </dependency>-->

        <!-- For now, fastutil is provided already by flink-runtime -->
        <dependency>
            <groupId>it.unimi.dsi</groupId>
            <artifactId>fastutil</artifactId>
            <version>8.2.1</version>
<!--            <scope>provided</scope>-->
        </dependency>

        <!-- Optional Parquet Builders for Formats like Avro, Protobuf, Thrift -->

        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-avro</artifactId>
            <version>${flink.format.parquet.version}</version>
<!--            <optional>true</optional>-->
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-client</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>it.unimi.dsi</groupId>
                    <artifactId>fastutil</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-parquet</artifactId>
            <version>1.8.0</version>
        </dependency>

        <!-- test dependencies -->

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-avro</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka-0.10_2.11</artifactId>
            <version>${flink.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.16.18</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.47</version>
        </dependency>

    </dependencies>
    <build>
        <plugins>
            <!-- Generate Test class from avro schema -->
            <plugin>
                <groupId>org.apache.avro</groupId>
                <artifactId>avro-maven-plugin</artifactId>
                <version>1.8.2</version>
                <executions>
                    <execution>
                        <phase>generate-sources</phase>
                        <goals>
                            <goal>schema</goal>
                        </goals>
                        <configuration>
                            <sourceDirectory>${project.basedir}/src/main/resources/</sourceDirectory>
                            <outputDirectory>${project.basedir}/src/main/java/</outputDirectory>
                        </configuration>
                    </execution>
                </executions>
            </plugin>

            <!-- skip dependency convergence due to Hadoop dependency -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-enforcer-plugin</artifactId>
                <executions>
                    <execution>
                        <id>dependency-convergence</id>
                        <goals>
                            <goal>enforce</goal>
                        </goals>
                        <configuration>
                            <skip>true</skip>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>